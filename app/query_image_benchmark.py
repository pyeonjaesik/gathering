"""
ê²€ìƒ‰ì–´ ê¸°ë°˜ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬.

- ê²€ìƒ‰ì–´ 1ê°œë¥¼ ë°›ì•„ SerpAPI(google_images)ë¡œ ì´ë¯¸ì§€ë¥¼ ê°€ëŠ¥í•œ í•œ ë§ì´ ìˆ˜ì§‘
- ê° ì´ë¯¸ì§€ URLì„ analyzeë¡œ ë¶„ì„
- ì›ì¬ë£Œ/í’ˆëª©ë³´ê³ ë²ˆí˜¸/ì„±ë¶„í‘œ/ì œí’ˆëª… ê²€ì¶œ ì—¬ë¶€ì™€ ê·¼ê±°ë¥¼ í„°ë¯¸ë„ì— ê³„ì¸µí˜•ìœ¼ë¡œ ì¶œë ¥
"""

from __future__ import annotations

import os
import re
import time
import threading
from concurrent.futures import FIRST_COMPLETED, ThreadPoolExecutor, wait
from dataclasses import dataclass
from typing import Any

import requests

from app.ingredient_analyzer import URLIngredientAnalyzer

SERPAPI_URL = "https://serpapi.com/search.json"
SERPAPI_TIMEOUT = 25
SERPAPI_RETRIES = 2
SERPAPI_RETRY_BACKOFF = 0.7


@dataclass
class ImageCandidate:
    url: str
    title: str | None
    source: str | None
    page_no: int
    rank_in_page: int


def _compact(text: Any) -> str:
    return re.sub(r"\s+", "", str(text or "")).strip()


def _short(text: Any, max_len: int = 42) -> str:
    value = _compact(text)
    if not value:
        return "null"
    if len(value) <= max_len:
        return value
    return value[: max_len - 1] + "â€¦"


def _is_transient_error(err: str | None) -> bool:
    low = str(err or "").lower()
    return any(
        k in low
        for k in (
            "429",
            "resource_exhausted",
            "timeout",
            "timed out",
            "deadline",
            "temporarily unavailable",
            "503",
            "502",
        )
    )


def _contains_nutrition(text: str | None) -> tuple[bool, str]:
    value = str(text or "")
    kws = ["ì˜ì–‘ì •ë³´", "ì˜ì–‘ì„±ë¶„", "ë‚˜íŠ¸ë¥¨", "íƒ„ìˆ˜í™”ë¬¼", "ë‹¨ë°±ì§ˆ", "ì§€ë°©", "calories", "nutrition"]
    found = [kw for kw in kws if kw.lower() in value.lower()]
    if not found:
        return (False, "ì˜ì–‘ì„±ë¶„ í‚¤ì›Œë“œ ë¯¸ê²€ì¶œ")
    return (True, f"ì˜ì–‘ì„±ë¶„ í‚¤ì›Œë“œ ê²€ì¶œ: {', '.join(found[:6])}")


def _mark_report(result: dict[str, Any]) -> str:
    raw = result.get("itemMnftrRptNo")
    compact = _compact(raw)
    if not compact:
        return "âŒ"
    qf = result.get("quality_flags") or {}
    digits = re.sub(r"[^0-9]", "", str(compact))
    if 10 <= len(digits) <= 16 and qf.get("report_number_complete") is True:
        return "âœ…"
    return "ğŸ”º"


def _mark_ingredients(result: dict[str, Any]) -> str:
    ing = _compact(result.get("ingredients_text"))
    if not ing:
        return "âŒ"
    qf = result.get("quality_flags") or {}
    if qf.get("ingredients_complete") is True:
        return "âœ…"
    if len(ing) >= 20 and ("," in ing or "ì›ì¬ë£Œ" in ing.lower() or "ingredients" in ing.lower()):
        return "âœ…"
    return "ğŸ”º"


def _mark_product(result: dict[str, Any], title: str | None) -> tuple[str, str | None]:
    name = result.get("product_name_in_image")
    value = _compact(name)
    if not value:
        return ("âŒ", None)
    qf = result.get("quality_flags") or {}
    if qf.get("product_name_complete") is True:
        return ("âœ…", value)
    if result.get("product_name_in_image") and len(value) >= 2:
        return ("âœ…", value)
    return ("ğŸ”º", value)


def _mark_nutrition(full_text: str | None) -> tuple[str, str]:
    has_nutri, how = _contains_nutrition(full_text)
    if not has_nutri:
        return ("âŒ", how)
    low = str(full_text or "").lower()
    hits = 0
    for kw in ("ì˜ì–‘ì •ë³´", "ì˜ì–‘ì„±ë¶„", "ë‚˜íŠ¸ë¥¨", "íƒ„ìˆ˜í™”ë¬¼", "ë‹¨ë°±ì§ˆ", "ì§€ë°©", "calories", "nutrition"):
        if kw.lower() in low:
            hits += 1
    if hits >= 2:
        return ("âœ…", how)
    return ("ğŸ”º", how)


def _mark_nutrition_from_result(result: dict[str, Any]) -> tuple[str, str]:
    txt = result.get("nutrition_text")
    if txt:
        compact = _compact(txt)
        if not compact:
            return ("âŒ", "nutrition_text empty")
        qf = result.get("quality_flags") or {}
        if qf.get("nutrition_complete") is True:
            return ("âœ…", "nutrition_complete=true")
        if len(compact) >= 12:
            return ("ğŸ”º", "nutrition_text partial")
        return ("ğŸ”º", "nutrition_text short")
    return ("âŒ", "nutrition_text null")


def _search_images_all(query: str, api_key: str, max_pages: int = 20, per_page: int = 100) -> list[ImageCandidate]:
    seen: set[str] = set()
    out: list[ImageCandidate] = []

    for page_no in range(max_pages):
        params = {
            "engine": "google_images",
            "q": query,
            "hl": "ko",
            "gl": "kr",
            "num": per_page,
            "ijn": page_no,
            "api_key": api_key,
            "no_cache": "true",
        }

        data: dict[str, Any] | None = None
        last_error = None
        for attempt in range(SERPAPI_RETRIES + 1):
            try:
                resp = requests.get(SERPAPI_URL, params=params, timeout=SERPAPI_TIMEOUT)
                data = resp.json()
                api_err = data.get("error")
                if resp.status_code == 200 and api_err is None:
                    break
                last_error = f"http={resp.status_code} api_error={api_err}"
                if resp.status_code in (429, 500, 502, 503, 504) and attempt < SERPAPI_RETRIES:
                    time.sleep(SERPAPI_RETRY_BACKOFF * (attempt + 1))
                    continue
                raise RuntimeError(last_error)
            except Exception as exc:  # pylint: disable=broad-except
                last_error = str(exc)
                if attempt < SERPAPI_RETRIES:
                    time.sleep(SERPAPI_RETRY_BACKOFF * (attempt + 1))
                    continue
                raise RuntimeError(f"SerpAPI ê²€ìƒ‰ ì‹¤íŒ¨(page={page_no}): {last_error}") from exc

        if data is None:
            break
        images = data.get("images_results") or []
        if not images:
            break

        added = 0
        for rank, item in enumerate(images, start=1):
            url = item.get("original") or item.get("thumbnail")
            if not url or url in seen:
                continue
            seen.add(url)
            out.append(
                ImageCandidate(
                    url=url,
                    title=item.get("title"),
                    source=item.get("source"),
                    page_no=page_no,
                    rank_in_page=rank,
                )
            )
            added += 1

        if added == 0:
            break

    return out


def run_query_image_benchmark(
    query: str,
    max_pages: int = 20,
    delay_sec: float = 0.0,
    max_concurrency: int = 5,
    adaptive: bool = True,
) -> None:
    serp_key = os.getenv("SERPAPI_KEY")
    if not serp_key:
        raise SystemExit("SERPAPI_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    gemini_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not gemini_key:
        raise SystemExit("GEMINI_API_KEY(ë˜ëŠ” GOOGLE_API_KEY) í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

    print("\n=== ê²€ìƒ‰ì–´ ê¸°ë°˜ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ ===")
    print(f"- ê²€ìƒ‰ì–´: {query}")
    print(f"- ìµœëŒ€ í˜ì´ì§€: {max_pages}")
    print("- SerpAPIì—ì„œ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
    images = _search_images_all(query=query, api_key=serp_key, max_pages=max_pages, per_page=100)
    print(f"- ìˆ˜ì§‘ëœ ì´ë¯¸ì§€: {len(images)}ê°œ")
    print("- ìƒíƒœ ê¸°ì¤€: âœ… ì˜¨ì „ ê²€ì¶œ | ğŸ”º ë¶€ë¶„/ë¶ˆí™•ì‹¤ | âŒ ë¯¸ê²€ì¶œ")
    if not images:
        return

    max_concurrency = max(1, min(10, int(max_concurrency)))
    print(f"- analyze ë³‘ë ¬ ì²˜ë¦¬: ìµœëŒ€ {max_concurrency}ê°œ ë™ì‹œ ì‹¤í–‰")
    print(f"- adaptive ëª¨ë“œ: {'ON' if adaptive else 'OFF'}")
    thread_local = threading.local()

    def _get_analyzer() -> URLIngredientAnalyzer:
        analyzer = getattr(thread_local, "analyzer", None)
        if analyzer is None:
            # ë²¤ì¹˜ë§ˆí¬ëŠ” ì²´ê° ì†ë„ë¥¼ ìœ„í•´ timeout/retryë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ë‚®ì¶˜ë‹¤.
            analyzer = URLIngredientAnalyzer(
                api_key=gemini_key,
                strict_mode=False,
                request_timeout_sec=35,
                download_timeout_sec=12,
                download_retries=1,
                model_retries=1,
            )
            thread_local.analyzer = analyzer
        return analyzer

    def _analyze_one(idx: int, img: ImageCandidate) -> tuple[int, ImageCandidate, dict[str, Any], str | None]:
        try:
            analyzer = _get_analyzer()
            result = analyzer.analyze(image_url=img.url, target_item_rpt_no=None)
            return (idx, img, result, None)
        except Exception as exc:  # pylint: disable=broad-except
            result = {
                "itemMnftrRptNo": None,
                "ingredients_text": None,
                "full_text": None,
                "note": f"analysis_error:{type(exc).__name__}",
            }
            return (idx, img, result, str(exc))

    ok_cnt = 0
    with ThreadPoolExecutor(max_workers=max_concurrency) as ex:
        pending: dict[Any, tuple[int, ImageCandidate]] = {}
        next_i = 0
        current_limit = min(3, max_concurrency) if adaptive else max_concurrency
        done_count = 0
        stable_success = 0
        last_heartbeat = time.time()
        while pending or next_i < len(images):
            while next_i < len(images) and len(pending) < current_limit:
                idx = next_i + 1
                img = images[next_i]
                fut = ex.submit(_analyze_one, idx, img)
                pending[fut] = (idx, img)
                next_i += 1

            if not pending:
                continue

            done_set, _ = wait(set(pending.keys()), timeout=2.0, return_when=FIRST_COMPLETED)
            now = time.time()
            if not done_set and (now - last_heartbeat) >= 2.0:
                print(
                    f"  ...ë¶„ì„ ì§„í–‰ì¤‘ ({done_count}/{len(images)} ì™„ë£Œ)"
                    f" | in_flight={len(pending)} | limit={current_limit}"
                )
                last_heartbeat = now
                continue

            for fut in done_set:
                idx, img = pending.pop(fut)
                done_count += 1
                idx, img, result, err = fut.result()
                rpt = result.get("itemMnftrRptNo")
                ing = result.get("ingredients_text")
                full_text = result.get("full_text")
                mark_report = _mark_report(result)
                mark_ing = _mark_ingredients(result)
                mark_prod, product_name = _mark_product(result, img.title)
                mark_nutri, nutri_how = _mark_nutrition_from_result(result)
                gate_pass = bool(result.get("quality_gate_pass"))
                gate_score = result.get("quality_score")
                gate_fail = result.get("quality_fail_reasons") or []
                gate_result = "READ" if (str(result.get("ai_decision") or "").upper() == "READ") else "SKIP"
                suitability = str(result.get("ai_suitability") or "").strip()
                if suitability not in ("ì í•©", "ë¶€ì í•©"):
                    suitability = "ì í•©" if gate_result == "READ" else "ë¶€ì í•©"
                decision_conf = result.get("ai_decision_confidence")
                decision_reason = result.get("ai_decision_reason")
                raw_model_text = result.get("raw_model_text")

                got_any = any(m != "âŒ" for m in (mark_report, mark_ing, mark_prod, mark_nutri))
                if got_any and err is None:
                    ok_cnt += 1

                print(f"\n[{idx:03d}/{len(images):03d}] URL: {img.url}")
                print("  [AI ì›ë¬¸ ì‘ë‹µ]")
                print(f"  {raw_model_text or '(ì›ë¬¸ ì—†ìŒ)'}")

                if adaptive:
                    if _is_transient_error(err):
                        prev = current_limit
                        current_limit = max(1, current_limit - 1)
                        stable_success = 0
                        if current_limit != prev:
                            print(f"  âš™ï¸ adaptive: ì¼ì‹œ ì˜¤ë¥˜ ê°ì§€ -> ë™ì‹œì„± {prev} -> {current_limit}")
                    else:
                        stable_success += 1
                        if stable_success >= 8 and current_limit < max_concurrency:
                            prev = current_limit
                            current_limit += 1
                            stable_success = 0
                            print(f"  âš™ï¸ adaptive: ì•ˆì • êµ¬ê°„ -> ë™ì‹œì„± {prev} -> {current_limit}")

                if delay_sec > 0:
                    time.sleep(delay_sec)

    print("\n" + "=" * 90)
    print("ìš”ì•½")
    print(f"- ì´ ì´ë¯¸ì§€: {len(images)}")
    print(f"- ì˜ë¯¸ ìˆëŠ” ì¶”ì¶œ(ë²ˆí˜¸/ì›ì¬ë£Œ/ì„±ë¶„í‘œ/ì œí’ˆëª… ì¤‘ í•˜ë‚˜ ì´ìƒ): {ok_cnt}")
    print(f"- ë¯¸ê²€ì¶œ/ì˜¤ë¥˜ ì¤‘ì‹¬ ì´ë¯¸ì§€: {len(images) - ok_cnt}")


def run_query_image_benchmark_interactive() -> None:
    print("\n  ğŸ” [ê²€ìƒ‰ì–´ ê¸°ë°˜ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬]")
    query = input("  ğŸ”¹ ê²€ìƒ‰ì–´ ì…ë ¥: ").strip()
    if not query:
        print("  âš ï¸ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    raw_pages = input("  ğŸ”¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ [ê¸°ë³¸ 20]: ").strip()
    max_pages = 20
    if raw_pages:
        try:
            v = int(raw_pages)
            if v > 0:
                max_pages = v
        except ValueError:
            pass

    raw_delay = input("  ğŸ”¹ ì´ë¯¸ì§€ ê°„ ëŒ€ê¸°(ì´ˆ) [ê¸°ë³¸ 0]: ").strip()
    delay_sec = 0.0
    if raw_delay:
        try:
            d = float(raw_delay)
            if d >= 0:
                delay_sec = d
        except ValueError:
            pass

    raw_conc = input("  ğŸ”¹ ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜ [ê¸°ë³¸ 5]: ").strip()
    max_concurrency = 5
    if raw_conc:
        try:
            v = int(raw_conc)
            if v > 0:
                max_concurrency = v
        except ValueError:
            pass

    raw_adapt = input("  ğŸ”¹ adaptive ìë™ ê°ì† ì‚¬ìš©? [Y/n]: ").strip().lower()
    adaptive = (raw_adapt != "n")

    print("\n  ğŸš€ ì‹¤í–‰í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ì´ë¯¸ì§€ë³„ë¡œ ìˆœì°¨ ì¶œë ¥ë©ë‹ˆë‹¤.")
    run_query_image_benchmark(
        query=query,
        max_pages=max_pages,
        delay_sec=delay_sec,
        max_concurrency=max_concurrency,
        adaptive=adaptive,
    )

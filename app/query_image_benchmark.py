"""
ê²€ìƒ‰ì–´ ê¸°ë°˜ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬.

- ê²€ìƒ‰ì–´ 1ê°œë¥¼ ë°›ì•„ SerpAPI(google_images)ë¡œ ì´ë¯¸ì§€ë¥¼ ê°€ëŠ¥í•œ í•œ ë§ì´ ìˆ˜ì§‘
- ê° ì´ë¯¸ì§€ URLì„ analyzeë¡œ ë¶„ì„
- ì›ì¬ë£Œ/í’ˆëª©ë³´ê³ ë²ˆí˜¸/ì„±ë¶„í‘œ/ì œí’ˆëª… ê²€ì¶œ ì—¬ë¶€ì™€ ê·¼ê±°ë¥¼ í„°ë¯¸ë„ì— ê³„ì¸µí˜•ìœ¼ë¡œ ì¶œë ¥
"""

from __future__ import annotations

import os
import re
import time
import threading
import json
from concurrent.futures import FIRST_COMPLETED, ThreadPoolExecutor, wait
from dataclasses import dataclass
from typing import Any

import requests

from app.analyzer import URLIngredientAnalyzer

SERPAPI_URL = "https://serpapi.com/search.json"
SERPAPI_TIMEOUT = 25
SERPAPI_RETRIES = 2
SERPAPI_RETRY_BACKOFF = 0.7


@dataclass
class ImageCandidate:
    url: str
    title: str | None
    source: str | None
    page_no: int
    rank_in_page: int


def _compact(text: Any) -> str:
    return re.sub(r"\s+", "", str(text or "")).strip()


def _short(text: Any, max_len: int = 42) -> str:
    value = _compact(text)
    if not value:
        return "null"
    if len(value) <= max_len:
        return value
    return value[: max_len - 1] + "â€¦"


def _extract_assistant_content(raw_api_response: Any, raw_model_text: Any) -> str:
    text = str(raw_api_response or "").strip()
    if text:
        try:
            payload = json.loads(text)
            choices = payload.get("choices") or []
            if choices:
                msg = (choices[0] or {}).get("message") or {}
                content = msg.get("content")
                if isinstance(content, str) and content.strip():
                    return content.strip()
        except Exception:  # pylint: disable=broad-except
            pass
    fallback = str(raw_model_text or "").strip()
    return fallback


def _is_precheck_skip(result: dict[str, Any]) -> bool:
    reason = str(result.get("ai_decision_reason") or "").lower()
    if "precheck" in reason:
        return True
    for code in (result.get("quality_fail_reasons") or []):
        if str(code).lower().startswith("precheck:"):
            return True
    return False


def _is_api_failure(result: dict[str, Any], err: str | None) -> bool:
    if err:
        return True
    reason = str(result.get("ai_decision_reason") or "").lower()
    note = str(result.get("note") or "").lower()
    text = " ".join([reason, note])
    failure_keys = (
        "openai_http_",
        "empty_model_response",
        "chatgpt analyze error",
        "resource_exhausted",
        "timeout",
        "insufficient_quota",
    )
    return any(k in text for k in failure_keys)


def _all_true_flags(result: dict[str, Any], keys: list[str]) -> bool:
    qf = result.get("quality_flags") or {}
    return all(qf.get(k) is True for k in keys)


def _is_transient_error(err: str | None) -> bool:
    low = str(err or "").lower()
    return any(
        k in low
        for k in (
            "429",
            "resource_exhausted",
            "timeout",
            "timed out",
            "deadline",
            "temporarily unavailable",
            "503",
            "502",
        )
    )


def _contains_nutrition(text: str | None) -> tuple[bool, str]:
    value = str(text or "")
    kws = ["ì˜ì–‘ì •ë³´", "ì˜ì–‘ì„±ë¶„", "ë‚˜íŠ¸ë¥¨", "íƒ„ìˆ˜í™”ë¬¼", "ë‹¨ë°±ì§ˆ", "ì§€ë°©", "calories", "nutrition"]
    found = [kw for kw in kws if kw.lower() in value.lower()]
    if not found:
        return (False, "ì˜ì–‘ì„±ë¶„ í‚¤ì›Œë“œ ë¯¸ê²€ì¶œ")
    return (True, f"ì˜ì–‘ì„±ë¶„ í‚¤ì›Œë“œ ê²€ì¶œ: {', '.join(found[:6])}")


def _mark_report(result: dict[str, Any]) -> str:
    raw = result.get("itemMnftrRptNo")
    compact = _compact(raw)
    if not compact:
        return "âŒ"
    qf = result.get("quality_flags") or {}
    digits = re.sub(r"[^0-9]", "", str(compact))
    if 10 <= len(digits) <= 16 and qf.get("report_number_complete") is True:
        return "âœ…"
    return "ğŸ”º"


def _mark_ingredients(result: dict[str, Any]) -> str:
    ing = _compact(result.get("ingredients_text"))
    if not ing:
        return "âŒ"
    qf = result.get("quality_flags") or {}
    if qf.get("ingredients_complete") is True:
        return "âœ…"
    if len(ing) >= 20 and ("," in ing or "ì›ì¬ë£Œ" in ing.lower() or "ingredients" in ing.lower()):
        return "âœ…"
    return "ğŸ”º"


def _mark_product(result: dict[str, Any], title: str | None) -> tuple[str, str | None]:
    name = result.get("product_name_in_image")
    value = _compact(name)
    if not value:
        return ("âŒ", None)
    qf = result.get("quality_flags") or {}
    if qf.get("product_name_complete") is True:
        return ("âœ…", value)
    if result.get("product_name_in_image") and len(value) >= 2:
        return ("âœ…", value)
    return ("ğŸ”º", value)


def _mark_nutrition(full_text: str | None) -> tuple[str, str]:
    has_nutri, how = _contains_nutrition(full_text)
    if not has_nutri:
        return ("âŒ", how)
    low = str(full_text or "").lower()
    hits = 0
    for kw in ("ì˜ì–‘ì •ë³´", "ì˜ì–‘ì„±ë¶„", "ë‚˜íŠ¸ë¥¨", "íƒ„ìˆ˜í™”ë¬¼", "ë‹¨ë°±ì§ˆ", "ì§€ë°©", "calories", "nutrition"):
        if kw.lower() in low:
            hits += 1
    if hits >= 2:
        return ("âœ…", how)
    return ("ğŸ”º", how)


def _mark_nutrition_from_result(result: dict[str, Any]) -> tuple[str, str]:
    txt = result.get("nutrition_text")
    if txt:
        compact = _compact(txt)
        if not compact:
            return ("âŒ", "nutrition_text empty")
        qf = result.get("quality_flags") or {}
        if qf.get("nutrition_complete") is True:
            return ("âœ…", "nutrition_complete=true")
        if len(compact) >= 12:
            return ("ğŸ”º", "nutrition_text partial")
        return ("ğŸ”º", "nutrition_text short")
    return ("âŒ", "nutrition_text null")


def _search_images_all(query: str, api_key: str, max_pages: int = 20, per_page: int = 100) -> list[ImageCandidate]:
    seen: set[str] = set()
    out: list[ImageCandidate] = []

    for page_no in range(max_pages):
        params = {
            "engine": "google_images",
            "q": query,
            "hl": "ko",
            "gl": "kr",
            "num": per_page,
            "ijn": page_no,
            "api_key": api_key,
            "no_cache": "true",
        }

        data: dict[str, Any] | None = None
        last_error = None
        for attempt in range(SERPAPI_RETRIES + 1):
            try:
                resp = requests.get(SERPAPI_URL, params=params, timeout=SERPAPI_TIMEOUT)
                data = resp.json()
                api_err = data.get("error")
                if resp.status_code == 200 and api_err is None:
                    break
                last_error = f"http={resp.status_code} api_error={api_err}"
                if resp.status_code in (429, 500, 502, 503, 504) and attempt < SERPAPI_RETRIES:
                    time.sleep(SERPAPI_RETRY_BACKOFF * (attempt + 1))
                    continue
                raise RuntimeError(last_error)
            except Exception as exc:  # pylint: disable=broad-except
                last_error = str(exc)
                if attempt < SERPAPI_RETRIES:
                    time.sleep(SERPAPI_RETRY_BACKOFF * (attempt + 1))
                    continue
                raise RuntimeError(f"SerpAPI ê²€ìƒ‰ ì‹¤íŒ¨(page={page_no}): {last_error}") from exc

        if data is None:
            break
        images = data.get("images_results") or []
        if not images:
            break

        added = 0
        for rank, item in enumerate(images, start=1):
            url = item.get("original") or item.get("thumbnail")
            if not url or url in seen:
                continue
            seen.add(url)
            out.append(
                ImageCandidate(
                    url=url,
                    title=item.get("title"),
                    source=item.get("source"),
                    page_no=page_no,
                    rank_in_page=rank,
                )
            )
            added += 1

        if added == 0:
            break

    return out


def run_query_image_benchmark(
    query: str,
    max_pages: int = 20,
    delay_sec: float = 0.0,
    max_concurrency: int = 5,
    adaptive: bool = True,
) -> None:
    serp_key = os.getenv("SERPAPI_KEY")
    if not serp_key:
        raise SystemExit("SERPAPI_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        raise SystemExit("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

    print("\n=== ê²€ìƒ‰ì–´ ê¸°ë°˜ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ ===")
    print(f"- ê²€ìƒ‰ì–´: {query}")
    print(f"- ìµœëŒ€ í˜ì´ì§€: {max_pages}")
    print("- SerpAPIì—ì„œ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
    images = _search_images_all(query=query, api_key=serp_key, max_pages=max_pages, per_page=100)
    print(f"- ìˆ˜ì§‘ëœ ì´ë¯¸ì§€: {len(images)}ê°œ")
    print("- ìƒíƒœ ê¸°ì¤€: âœ… ì¶”ì¶œ ê°€ëŠ¥ | âŒ ì¶”ì¶œ ë¶ˆê°€")
    if not images:
        return

    max_concurrency = max(1, min(10, int(max_concurrency)))
    print(f"- analyze ë³‘ë ¬ ì²˜ë¦¬: ìµœëŒ€ {max_concurrency}ê°œ ë™ì‹œ ì‹¤í–‰")
    print(f"- adaptive ëª¨ë“œ: {'ON' if adaptive else 'OFF'}")
    thread_local = threading.local()

    def _get_analyzer() -> URLIngredientAnalyzer:
        analyzer = getattr(thread_local, "analyzer", None)
        if analyzer is None:
            # ë²¤ì¹˜ë§ˆí¬ëŠ” ì²´ê° ì†ë„ë¥¼ ìœ„í•´ timeout/retryë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ë‚®ì¶˜ë‹¤.
            analyzer = URLIngredientAnalyzer(
                api_key=openai_key,
                strict_mode=False,
                request_timeout_sec=35,
                download_timeout_sec=12,
                download_retries=1,
                model_retries=1,
            )
            thread_local.analyzer = analyzer
        return analyzer

    def _analyze_one(idx: int, img: ImageCandidate) -> tuple[int, ImageCandidate, dict[str, Any], str | None]:
        try:
            analyzer = _get_analyzer()
            result = analyzer.analyze_pass2(image_url=img.url, target_item_rpt_no=None)
            return (idx, img, result, None)
        except Exception as exc:  # pylint: disable=broad-except
            result = {
                "itemMnftrRptNo": None,
                "ingredients_text": None,
                "full_text": None,
                "note": f"analysis_error:{type(exc).__name__}",
            }
            return (idx, img, result, str(exc))

    extractable_cnt = 0
    precheck_skip_cnt = 0
    api_fail_cnt = 0
    api_success_skip_cnt = 0
    api_success_read_cnt = 0
    all_true_except_ing_cnt = 0
    all_true_with_ing_cnt = 0
    with ThreadPoolExecutor(max_workers=max_concurrency) as ex:
        pending: dict[Any, tuple[int, ImageCandidate]] = {}
        next_i = 0
        current_limit = min(3, max_concurrency) if adaptive else max_concurrency
        done_count = 0
        stable_success = 0
        last_heartbeat = time.time()
        while pending or next_i < len(images):
            while next_i < len(images) and len(pending) < current_limit:
                idx = next_i + 1
                img = images[next_i]
                fut = ex.submit(_analyze_one, idx, img)
                pending[fut] = (idx, img)
                next_i += 1

            if not pending:
                continue

            done_set, _ = wait(set(pending.keys()), timeout=2.0, return_when=FIRST_COMPLETED)
            now = time.time()
            if not done_set and (now - last_heartbeat) >= 2.0:
                print(
                    f"  ...ë¶„ì„ ì§„í–‰ì¤‘ ({done_count}/{len(images)} ì™„ë£Œ)"
                    f" | in_flight={len(pending)} | limit={current_limit}"
                )
                last_heartbeat = now
                continue

            for fut in done_set:
                idx, img = pending.pop(fut)
                done_count += 1
                idx, img, result, err = fut.result()
                gate_pass = bool(result.get("quality_gate_pass"))
                gate_result = "READ" if (str(result.get("ai_decision") or "").upper() == "READ") else "SKIP"
                is_extractable = gate_pass and (gate_result == "READ")
                if is_extractable and err is None:
                    extractable_cnt += 1

                # í†µê³„ ë¶„ë¥˜
                is_precheck = _is_precheck_skip(result)
                is_api_fail = _is_api_failure(result, err)
                if is_precheck:
                    precheck_skip_cnt += 1
                elif is_api_fail:
                    api_fail_cnt += 1
                else:
                    if gate_result == "READ":
                        api_success_read_cnt += 1
                    else:
                        api_success_skip_cnt += 1

                # í’ˆì§ˆ í”Œë˜ê·¸ í†µê³„
                relaxed_keys = [
                    "is_clear_text",
                    "is_full_frame",
                    "is_flat_undistorted",
                    "has_report_number_label",
                    "has_product_name",
                    "has_single_product",
                    "has_nutrition_section",
                ]
                strict_keys = relaxed_keys + ["has_ingredients_section"]
                if _all_true_flags(result, relaxed_keys):
                    all_true_except_ing_cnt += 1
                if _all_true_flags(result, strict_keys):
                    all_true_with_ing_cnt += 1

                print(f"\n[{idx:03d}/{len(images):03d}] URL: {img.url}")
                print("  [AI ì›ë¬¸ ì‘ë‹µ]")
                if err:
                    print(f"  (í˜¸ì¶œ ì‹¤íŒ¨) {err}")
                else:
                    content = _extract_assistant_content(
                        raw_api_response=result.get("raw_api_response"),
                        raw_model_text=result.get("raw_model_text"),
                    )
                    if not content:
                        content = result.get("ai_decision_reason") or result.get("note") or "(ì›ë¬¸ ì—†ìŒ)"
                    print(f"  {content}")

                if adaptive:
                    if _is_transient_error(err):
                        prev = current_limit
                        current_limit = max(1, current_limit - 1)
                        stable_success = 0
                        if current_limit != prev:
                            print(f"  âš™ï¸ adaptive: ì¼ì‹œ ì˜¤ë¥˜ ê°ì§€ -> ë™ì‹œì„± {prev} -> {current_limit}")
                    else:
                        stable_success += 1
                        if stable_success >= 8 and current_limit < max_concurrency:
                            prev = current_limit
                            current_limit += 1
                            stable_success = 0
                            print(f"  âš™ï¸ adaptive: ì•ˆì • êµ¬ê°„ -> ë™ì‹œì„± {prev} -> {current_limit}")

                if delay_sec > 0:
                    time.sleep(delay_sec)

    print("\n" + "=" * 90)
    print("ìš”ì•½")
    print(f"- ì´ ì´ë¯¸ì§€: {len(images)}")
    print(f"- ì¶”ì¶œ ê°€ëŠ¥: {extractable_cnt}")
    print(f"- ì¶”ì¶œ ë¶ˆê°€: {len(images) - extractable_cnt}")
    print("\ní’ˆì§ˆ ì¡°ê±´ í†µê³„")
    print(f"- has_ingredients_section ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ í•µì‹¬ê°’ ëª¨ë‘ true: {all_true_except_ing_cnt}")
    print(f"- has_ingredients_section í¬í•¨í•´ì„œ í•µì‹¬ê°’ ëª¨ë‘ true: {all_true_with_ing_cnt}")
    print("\nì‹¤í–‰ ìƒíƒœ í†µê³„")
    print(f"- Pass-1(ì‚¬ì „ê²€ì¦)ì—ì„œ ì œì™¸: {precheck_skip_cnt}")
    print(f"- API í˜¸ì¶œ ì‹¤íŒ¨: {api_fail_cnt}")
    print(f"- API í˜¸ì¶œ ì„±ê³µ + ë¶€ì í•©(SKIP): {api_success_skip_cnt}")
    print(f"- API í˜¸ì¶œ ì„±ê³µ + ì í•©(READ): {api_success_read_cnt}")


def run_query_image_benchmark_interactive() -> None:
    print("\n  ğŸ” [ê²€ìƒ‰ì–´ ê¸°ë°˜ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬]")
    query = input("  ğŸ”¹ ê²€ìƒ‰ì–´ ì…ë ¥: ").strip()
    if not query:
        print("  âš ï¸ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    raw_pages = input("  ğŸ”¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ [ê¸°ë³¸ 20]: ").strip()
    max_pages = 20
    if raw_pages:
        try:
            v = int(raw_pages)
            if v > 0:
                max_pages = v
        except ValueError:
            pass

    # ìš”ì²­ì‚¬í•­: ì´ë¯¸ì§€ ê°„ ëŒ€ê¸° ê¸°ë³¸ê°’ 0 ê³ ì •
    delay_sec = 0.0

    raw_conc = input("  ğŸ”¹ ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜ [ê¸°ë³¸ 5]: ").strip()
    max_concurrency = 5
    if raw_conc:
        try:
            v = int(raw_conc)
            if v > 0:
                max_concurrency = v
        except ValueError:
            pass

    # ìš”ì²­ì‚¬í•­: adaptive ìë™ ê°ì† ê¸°ëŠ¥ OFF ê³ ì •
    adaptive = False

    print("\n  ğŸš€ ì‹¤í–‰í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ì´ë¯¸ì§€ë³„ë¡œ ìˆœì°¨ ì¶œë ¥ë©ë‹ˆë‹¤.")
    run_query_image_benchmark(
        query=query,
        max_pages=max_pages,
        delay_sec=delay_sec,
        max_concurrency=max_concurrency,
        adaptive=adaptive,
    )
